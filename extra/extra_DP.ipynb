{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. Data Loading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total split files loaded: 87038\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def parse_cell_file(content):\n",
    "\n",
    "    #Parses the content of a .cell file and converts it into a structured data format.\n",
    "    features = []\n",
    "    for line in content.strip().splitlines():\n",
    "        if not line.strip():\n",
    "            continue\n",
    "        try:\n",
    "            timestamp, direction, size = map(float, line.split())\n",
    "            signed_size = size if direction > 0 else -size\n",
    "            features.append([timestamp, signed_size])\n",
    "        except ValueError:\n",
    "            continue\n",
    "    return np.array(features)\n",
    "\n",
    "def load_mon_data(mon_folder):\n",
    "    \n",
    "    #Loads 'split' files from the 'mon' folder and labels them.\n",
    "    instances = []\n",
    "    labels = []\n",
    "\n",
    "    for file in os.listdir(mon_folder):\n",
    "        if \"split\" in file:\n",
    "            file_path = os.path.join(mon_folder, file)\n",
    "            try:\n",
    "                class_label = int(file.split('-')[0])  # Extract class label from the filename\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "            with open(file_path, 'r') as f:\n",
    "                instance = parse_cell_file(f.read())\n",
    "                if instance.size > 0:\n",
    "                    instances.append(instance)\n",
    "                    labels.append(class_label)\n",
    "\n",
    "    print(f\"Total split files loaded: {len(instances)}\")\n",
    "\n",
    "    return np.array(instances, dtype=object), np.array(labels)\n",
    "\n",
    "# data path\n",
    "mon_folder_path = './mon/ts'\n",
    "\n",
    "# Load data\n",
    "X_raw, y = load_mon_data(mon_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2. Feature Engineering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(X_raw):\n",
    "    feature_matrix = []\n",
    "\n",
    "    for instance in X_raw:\n",
    "        timestamps = instance[:, 0]\n",
    "        signed_sizes = instance[:, 1]\n",
    "\n",
    "        # Compute Features\n",
    "        packet_size_direction = np.sum(signed_sizes)\n",
    "        cumulative_packet_size = np.sum(np.abs(signed_sizes))\n",
    "        burst_lengths = len(signed_sizes)\n",
    "        num_incoming_packets = np.sum(signed_sizes > 0)\n",
    "        num_outgoing_packets = np.sum(signed_sizes < 0)\n",
    "        ratio_incoming_packets = (\n",
    "            num_incoming_packets / burst_lengths if burst_lengths > 0 else 0\n",
    "        )\n",
    "\n",
    "        if len(timestamps) > 1:\n",
    "            time_intervals = np.diff(timestamps)\n",
    "            mean_time_intervals = np.mean(time_intervals)\n",
    "            std_time_intervals = np.std(time_intervals)\n",
    "        else:\n",
    "            mean_time_intervals = 0\n",
    "            std_time_intervals = 0\n",
    "\n",
    "        # Feature vector\n",
    "        feature_vector = [\n",
    "            packet_size_direction,\n",
    "            cumulative_packet_size,\n",
    "            burst_lengths,\n",
    "            num_incoming_packets,\n",
    "            num_outgoing_packets,\n",
    "            ratio_incoming_packets,\n",
    "            mean_time_intervals,\n",
    "            std_time_intervals,\n",
    "        ]\n",
    "        feature_matrix.append(feature_vector)\n",
    "\n",
    "    return np.array(feature_matrix)\n",
    "\n",
    "X = create_features(X_raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total instances loaded: 87038\n",
      "Unique labels: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94]\n",
      "Feature matrix shape: (87038, 8)\n",
      "Class distribution:\n",
      "Class 0: 956 instances\n",
      "Class 1: 915 instances\n",
      "Class 2: 956 instances\n",
      "Class 3: 913 instances\n",
      "Class 4: 900 instances\n",
      "Class 5: 968 instances\n",
      "Class 6: 968 instances\n",
      "Class 7: 844 instances\n",
      "Class 8: 918 instances\n",
      "Class 9: 958 instances\n",
      "Class 10: 948 instances\n",
      "Class 11: 982 instances\n",
      "Class 12: 840 instances\n",
      "Class 13: 949 instances\n",
      "Class 14: 858 instances\n",
      "Class 15: 895 instances\n",
      "Class 16: 886 instances\n",
      "Class 17: 952 instances\n",
      "Class 18: 859 instances\n",
      "Class 19: 977 instances\n",
      "Class 20: 810 instances\n",
      "Class 21: 969 instances\n",
      "Class 22: 981 instances\n",
      "Class 23: 792 instances\n",
      "Class 24: 910 instances\n",
      "Class 25: 718 instances\n",
      "Class 26: 655 instances\n",
      "Class 27: 965 instances\n",
      "Class 28: 841 instances\n",
      "Class 29: 901 instances\n",
      "Class 30: 590 instances\n",
      "Class 31: 924 instances\n",
      "Class 32: 978 instances\n",
      "Class 33: 982 instances\n",
      "Class 34: 988 instances\n",
      "Class 35: 952 instances\n",
      "Class 36: 963 instances\n",
      "Class 37: 940 instances\n",
      "Class 38: 927 instances\n",
      "Class 39: 964 instances\n",
      "Class 40: 954 instances\n",
      "Class 41: 962 instances\n",
      "Class 42: 984 instances\n",
      "Class 43: 864 instances\n",
      "Class 44: 913 instances\n",
      "Class 45: 972 instances\n",
      "Class 46: 945 instances\n",
      "Class 47: 965 instances\n",
      "Class 48: 988 instances\n",
      "Class 49: 844 instances\n",
      "Class 50: 958 instances\n",
      "Class 51: 970 instances\n",
      "Class 52: 983 instances\n",
      "Class 53: 891 instances\n",
      "Class 54: 937 instances\n",
      "Class 55: 968 instances\n",
      "Class 56: 802 instances\n",
      "Class 57: 845 instances\n",
      "Class 58: 909 instances\n",
      "Class 59: 970 instances\n",
      "Class 60: 962 instances\n",
      "Class 61: 947 instances\n",
      "Class 62: 983 instances\n",
      "Class 63: 975 instances\n",
      "Class 64: 984 instances\n",
      "Class 65: 954 instances\n",
      "Class 66: 992 instances\n",
      "Class 67: 933 instances\n",
      "Class 68: 945 instances\n",
      "Class 69: 988 instances\n",
      "Class 70: 727 instances\n",
      "Class 71: 973 instances\n",
      "Class 72: 959 instances\n",
      "Class 73: 987 instances\n",
      "Class 74: 969 instances\n",
      "Class 75: 580 instances\n",
      "Class 76: 817 instances\n",
      "Class 77: 975 instances\n",
      "Class 78: 951 instances\n",
      "Class 79: 979 instances\n",
      "Class 80: 927 instances\n",
      "Class 81: 931 instances\n",
      "Class 82: 703 instances\n",
      "Class 83: 875 instances\n",
      "Class 84: 944 instances\n",
      "Class 85: 965 instances\n",
      "Class 86: 701 instances\n",
      "Class 87: 974 instances\n",
      "Class 88: 964 instances\n",
      "Class 89: 975 instances\n",
      "Class 90: 951 instances\n",
      "Class 91: 953 instances\n",
      "Class 92: 918 instances\n",
      "Class 93: 883 instances\n",
      "Class 94: 978 instances\n"
     ]
    }
   ],
   "source": [
    "# Verify data loading and feature extraction\n",
    "print(f\"Total instances loaded: {len(X_raw)}\")\n",
    "print(f\"Unique labels: {np.unique(y)}\") \n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "\n",
    "# Check data distribution by class\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "print(\"Class distribution:\")\n",
    "for label, count in zip(unique, counts):\n",
    "    print(f\"Class {label}: {count} instances\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3. Data Splitting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Distribution:\n",
      "66    694\n",
      "69    692\n",
      "48    692\n",
      "34    692\n",
      "73    691\n",
      "     ... \n",
      "82    492\n",
      "86    491\n",
      "26    459\n",
      "30    413\n",
      "75    406\n",
      "Name: count, Length: 95, dtype: int64\n",
      "Test Data Distribution:\n",
      "66    298\n",
      "73    296\n",
      "69    296\n",
      "34    296\n",
      "48    296\n",
      "     ... \n",
      "82    211\n",
      "86    210\n",
      "26    196\n",
      "30    177\n",
      "75    174\n",
      "Name: count, Length: 95, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Data Distribution\n",
    "print(\"Training Data Distribution:\")\n",
    "print(pd.Series(y_train).value_counts())\n",
    "print(\"Test Data Distribution:\")\n",
    "print(pd.Series(y_test).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4. Scaling the Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **5. Model training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Check the number of classes\n",
    "num_classes = len(np.unique(y))\n",
    "\n",
    "# One-hot encoding (required for Keras models)\n",
    "y_train_categorical = to_categorical(y_train, num_classes=num_classes)\n",
    "y_test_categorical = to_categorical(y_test, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m952/952\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 679us/step - accuracy: 0.0146 - loss: 4.5011\n",
      "Epoch 2/10\n",
      "\u001b[1m952/952\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 709us/step - accuracy: 0.0232 - loss: 4.3951\n",
      "Epoch 3/10\n",
      "\u001b[1m952/952\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 726us/step - accuracy: 0.0274 - loss: 4.3701\n",
      "Epoch 4/10\n",
      "\u001b[1m952/952\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 672us/step - accuracy: 0.0293 - loss: 4.3512\n",
      "Epoch 5/10\n",
      "\u001b[1m952/952\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 671us/step - accuracy: 0.0310 - loss: 4.3402\n",
      "Epoch 6/10\n",
      "\u001b[1m952/952\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 773us/step - accuracy: 0.0335 - loss: 4.3310\n",
      "Epoch 7/10\n",
      "\u001b[1m952/952\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.0353 - loss: 4.3253\n",
      "Epoch 8/10\n",
      "\u001b[1m952/952\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 773us/step - accuracy: 0.0339 - loss: 4.3170\n",
      "Epoch 9/10\n",
      "\u001b[1m952/952\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 833us/step - accuracy: 0.0344 - loss: 4.3100\n",
      "Epoch 10/10\n",
      "\u001b[1m952/952\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 692us/step - accuracy: 0.0374 - loss: 4.3031\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2b1820980>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "# Define the Deep Fingerprinting (DF) model\n",
    "def create_df_model(input_shape, num_classes):\n",
    "    model = Sequential([\n",
    "        Dense(256, activation='relu', input_shape=(input_shape,)),  # Input layer with 256 neurons\n",
    "        Dropout(0.5),  # Dropout to prevent overfitting\n",
    "        Dense(128, activation='relu'),  # Hidden layer with 128 neurons\n",
    "        Dropout(0.3),  # Dropout to prevent overfitting\n",
    "        Dense(64, activation='relu'),  # Hidden layer with 64 neurons\n",
    "        Dropout(0.3),  # Dropout to prevent overfitting\n",
    "        Dense(num_classes, activation='softmax')  # Output layer with softmax activation for multi-class classification\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])  # Compile the model\n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "df_model = create_df_model(X_train.shape[1], num_classes)\n",
    "\n",
    "# Train the model\n",
    "df_model.fit(X_train, y_train_categorical, epochs=10, batch_size=64, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **6. Model Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model...\n",
      "\u001b[1m816/816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 211us/step\n",
      "Accuracy: 3.9292279411764706\n",
      "Confusion Matrix:\n",
      "[[0 0 2 ... 0 0 0]\n",
      " [0 0 3 ... 0 0 0]\n",
      " [0 0 3 ... 0 0 0]\n",
      " ...\n",
      " [0 0 1 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 3 ... 0 0 1]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       287\n",
      "           1       0.00      0.00      0.00       275\n",
      "           2       0.02      0.01      0.01       287\n",
      "           3       0.00      0.00      0.00       274\n",
      "           4       0.00      0.00      0.00       270\n",
      "           5       0.08      0.00      0.01       290\n",
      "           6       0.02      0.00      0.01       290\n",
      "           7       0.04      0.02      0.03       253\n",
      "           8       0.00      0.00      0.00       275\n",
      "           9       0.00      0.00      0.00       287\n",
      "          10       0.00      0.00      0.00       284\n",
      "          11       0.06      0.27      0.10       295\n",
      "          12       0.04      0.12      0.06       252\n",
      "          13       0.00      0.00      0.00       285\n",
      "          14       0.00      0.00      0.00       257\n",
      "          15       0.03      0.01      0.02       269\n",
      "          16       0.00      0.00      0.00       266\n",
      "          17       0.00      0.00      0.00       286\n",
      "          18       0.00      0.00      0.00       258\n",
      "          19       0.00      0.00      0.00       293\n",
      "          20       0.05      0.09      0.07       243\n",
      "          21       0.00      0.00      0.00       291\n",
      "          22       0.00      0.00      0.00       294\n",
      "          23       0.04      0.02      0.02       238\n",
      "          24       0.04      0.03      0.04       273\n",
      "          25       0.00      0.00      0.00       215\n",
      "          26       0.10      0.09      0.09       196\n",
      "          27       0.00      0.00      0.00       290\n",
      "          28       0.00      0.00      0.00       252\n",
      "          29       0.00      0.00      0.00       270\n",
      "          30       0.09      0.40      0.14       177\n",
      "          31       0.00      0.00      0.00       277\n",
      "          32       0.00      0.00      0.00       293\n",
      "          33       0.05      0.04      0.04       295\n",
      "          34       0.00      0.00      0.00       296\n",
      "          35       0.00      0.00      0.00       286\n",
      "          36       0.00      0.00      0.00       289\n",
      "          37       0.00      0.00      0.00       282\n",
      "          38       0.08      0.09      0.08       278\n",
      "          39       0.00      0.00      0.00       289\n",
      "          40       0.03      0.10      0.05       286\n",
      "          41       0.05      0.01      0.02       289\n",
      "          42       0.00      0.00      0.00       295\n",
      "          43       0.00      0.00      0.00       259\n",
      "          44       0.03      0.18      0.06       274\n",
      "          45       0.00      0.00      0.00       292\n",
      "          46       0.00      0.00      0.00       284\n",
      "          47       0.00      0.00      0.00       290\n",
      "          48       0.00      0.00      0.00       296\n",
      "          49       0.00      0.00      0.00       253\n",
      "          50       0.00      0.00      0.00       287\n",
      "          51       0.00      0.00      0.00       291\n",
      "          52       0.00      0.00      0.00       295\n",
      "          53       0.00      0.00      0.00       267\n",
      "          54       0.00      0.00      0.00       281\n",
      "          55       0.00      0.00      0.00       290\n",
      "          56       0.05      0.06      0.05       241\n",
      "          57       0.03      0.04      0.04       253\n",
      "          58       0.02      0.13      0.04       273\n",
      "          59       0.02      0.05      0.03       291\n",
      "          60       0.00      0.00      0.00       289\n",
      "          61       0.00      0.00      0.00       284\n",
      "          62       0.03      0.04      0.03       295\n",
      "          63       0.00      0.00      0.00       293\n",
      "          64       0.04      0.04      0.04       295\n",
      "          65       0.03      0.10      0.04       286\n",
      "          66       0.06      0.22      0.09       298\n",
      "          67       0.00      0.00      0.00       280\n",
      "          68       0.00      0.00      0.00       284\n",
      "          69       0.11      0.01      0.02       296\n",
      "          70       0.03      0.35      0.05       218\n",
      "          71       0.00      0.00      0.00       292\n",
      "          72       0.00      0.00      0.00       288\n",
      "          73       0.04      0.32      0.07       296\n",
      "          74       0.04      0.08      0.06       291\n",
      "          75       0.04      0.34      0.07       174\n",
      "          76       0.00      0.00      0.00       245\n",
      "          77       0.00      0.00      0.00       293\n",
      "          78       0.02      0.04      0.03       285\n",
      "          79       0.02      0.06      0.04       294\n",
      "          80       0.05      0.06      0.06       278\n",
      "          81       0.00      0.00      0.00       279\n",
      "          82       0.16      0.11      0.13       211\n",
      "          83       0.00      0.00      0.00       263\n",
      "          84       0.03      0.22      0.05       283\n",
      "          85       0.00      0.00      0.00       290\n",
      "          86       0.17      0.07      0.10       210\n",
      "          87       0.00      0.00      0.00       292\n",
      "          88       0.00      0.00      0.00       289\n",
      "          89       0.00      0.00      0.00       293\n",
      "          90       0.03      0.21      0.06       285\n",
      "          91       0.00      0.00      0.00       286\n",
      "          92       0.00      0.00      0.00       275\n",
      "          93       0.00      0.00      0.00       265\n",
      "          94       0.14      0.00      0.01       293\n",
      "\n",
      "    accuracy                           0.04     26112\n",
      "   macro avg       0.02      0.04      0.02     26112\n",
      "weighted avg       0.02      0.04      0.02     26112\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Evaluating the model...\")\n",
    "y_pred_probs = df_model.predict(X_test)  # Predict probabilities for each class\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)  # Convert probabilities to class labels\n",
    "\n",
    "# Display evaluation metrics\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred) * 100)\n",
    "print(f\"Confusion Matrix:\\n{confusion_matrix(y_test, y_pred)}\")\n",
    "print(f\"Classification Report:\\n{classification_report(y_test, y_pred)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
