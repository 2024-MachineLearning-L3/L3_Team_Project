{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Total sites: 950\n",
      "Total samples: 19000\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Load the pickle files\n",
    "print(\"Loading data...\")\n",
    "with open(\"/Users/claire/Downloads/기계학습/mon_standard.pkl\", 'rb') as fi:  # Monitored 데이터 로드\n",
    "    mon_data = pickle.load(fi)\n",
    "\n",
    "# 데이터 확인\n",
    "print(\"Total sites:\", len(mon_data))  # 사이트 개수\n",
    "total_samples = sum(len(samples) for samples in mon_data.values())\n",
    "print(\"Total samples:\", total_samples)  # 전체 샘플 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6605263157894737\n",
      "Confusion Matrix:\n",
      " [[3 0 0 ... 0 0 0]\n",
      " [0 3 0 ... 0 0 0]\n",
      " [0 0 4 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 4 0 0]\n",
      " [0 0 0 ... 0 4 0]\n",
      " [0 0 0 ... 0 1 2]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.75      0.75         4\n",
      "           1       0.60      0.75      0.67         4\n",
      "           2       0.80      1.00      0.89         4\n",
      "           3       0.80      1.00      0.89         4\n",
      "           4       1.00      0.75      0.86         4\n",
      "           5       1.00      0.50      0.67         4\n",
      "           6       0.67      0.50      0.57         4\n",
      "           7       0.50      0.75      0.60         4\n",
      "           8       0.75      0.75      0.75         4\n",
      "           9       0.75      0.75      0.75         4\n",
      "          10       0.50      0.25      0.33         4\n",
      "          11       0.50      0.25      0.33         4\n",
      "          12       0.33      0.25      0.29         4\n",
      "          13       0.40      0.50      0.44         4\n",
      "          14       0.67      0.50      0.57         4\n",
      "          15       0.50      0.50      0.50         4\n",
      "          16       0.60      0.75      0.67         4\n",
      "          17       0.75      0.75      0.75         4\n",
      "          18       1.00      0.75      0.86         4\n",
      "          19       0.67      0.50      0.57         4\n",
      "          20       1.00      1.00      1.00         4\n",
      "          21       0.33      0.50      0.40         4\n",
      "          22       0.67      0.50      0.57         4\n",
      "          23       0.33      0.25      0.29         4\n",
      "          24       0.00      0.00      0.00         4\n",
      "          25       0.80      1.00      0.89         4\n",
      "          26       0.40      0.50      0.44         4\n",
      "          27       0.33      0.50      0.40         4\n",
      "          28       0.40      1.00      0.57         4\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       1.00      0.75      0.86         4\n",
      "          31       0.67      0.50      0.57         4\n",
      "          32       0.50      0.50      0.50         4\n",
      "          33       1.00      1.00      1.00         4\n",
      "          34       1.00      0.25      0.40         4\n",
      "          35       0.67      1.00      0.80         4\n",
      "          36       0.67      0.50      0.57         4\n",
      "          37       0.33      0.25      0.29         4\n",
      "          38       0.17      0.25      0.20         4\n",
      "          39       0.25      0.25      0.25         4\n",
      "          40       0.33      0.25      0.29         4\n",
      "          41       0.14      0.25      0.18         4\n",
      "          42       1.00      0.75      0.86         4\n",
      "          43       1.00      1.00      1.00         4\n",
      "          44       0.57      1.00      0.73         4\n",
      "          45       0.43      0.75      0.55         4\n",
      "          46       0.67      0.50      0.57         4\n",
      "          47       0.50      0.25      0.33         4\n",
      "          48       0.43      0.75      0.55         4\n",
      "          49       0.00      0.00      0.00         4\n",
      "          50       1.00      0.50      0.67         4\n",
      "          51       1.00      1.00      1.00         4\n",
      "          52       0.75      0.75      0.75         4\n",
      "          53       0.67      1.00      0.80         4\n",
      "          54       1.00      1.00      1.00         4\n",
      "          55       1.00      0.75      0.86         4\n",
      "          56       1.00      1.00      1.00         4\n",
      "          57       1.00      1.00      1.00         4\n",
      "          58       0.75      0.75      0.75         4\n",
      "          59       0.80      1.00      0.89         4\n",
      "          60       1.00      1.00      1.00         4\n",
      "          61       0.67      0.50      0.57         4\n",
      "          62       1.00      0.75      0.86         4\n",
      "          63       0.43      0.75      0.55         4\n",
      "          64       1.00      0.25      0.40         4\n",
      "          65       0.50      0.75      0.60         4\n",
      "          66       1.00      1.00      1.00         4\n",
      "          67       1.00      1.00      1.00         4\n",
      "          68       0.60      0.75      0.67         4\n",
      "          69       0.50      0.25      0.33         4\n",
      "          70       0.80      1.00      0.89         4\n",
      "          71       0.50      0.75      0.60         4\n",
      "          72       1.00      0.75      0.86         4\n",
      "          73       1.00      0.75      0.86         4\n",
      "          74       1.00      0.50      0.67         4\n",
      "          75       0.00      0.00      0.00         4\n",
      "          76       0.67      0.50      0.57         4\n",
      "          77       0.67      1.00      0.80         4\n",
      "          78       0.60      0.75      0.67         4\n",
      "          79       0.00      0.00      0.00         4\n",
      "          80       0.80      1.00      0.89         4\n",
      "          81       0.75      0.75      0.75         4\n",
      "          82       0.67      0.50      0.57         4\n",
      "          83       0.67      1.00      0.80         4\n",
      "          84       0.75      0.75      0.75         4\n",
      "          85       0.75      0.75      0.75         4\n",
      "          86       1.00      0.75      0.86         4\n",
      "          87       1.00      1.00      1.00         4\n",
      "          88       1.00      1.00      1.00         4\n",
      "          89       0.50      0.50      0.50         4\n",
      "          90       0.80      1.00      0.89         4\n",
      "          91       1.00      1.00      1.00         4\n",
      "          92       1.00      1.00      1.00         4\n",
      "          93       0.80      1.00      0.89         4\n",
      "          94       0.67      0.50      0.57         4\n",
      "\n",
      "    accuracy                           0.66       380\n",
      "   macro avg       0.68      0.66      0.65       380\n",
      "weighted avg       0.68      0.66      0.65       380\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier, StackingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 데이터 전처리 함수\n",
    "def process_data_multi_class(data, max_monitored_labels=95):\n",
    "    X1, X2, y = [], [], []\n",
    "\n",
    "    for site_id, samples in enumerate(data.values()):\n",
    "        if site_id >= max_monitored_labels:\n",
    "            break\n",
    "        for sample in samples:\n",
    "            X1.append([abs(c) for c in sample])\n",
    "            X2.append([(1 if c > 0 else -1) * 512 for c in sample])\n",
    "            y.append(site_id)\n",
    "\n",
    "    return X1, X2, y\n",
    "\n",
    "# Monitored 데이터 처리\n",
    "X1, X2, y = process_data_multi_class(mon_data, max_monitored_labels=95)\n",
    "\n",
    "# 시간 간격 평균(mean) 계산 함수 수정\n",
    "def calculate_mean_time_intervals(X1):\n",
    "    mean_intervals = []\n",
    "    for sample in X1:\n",
    "        if len(sample) > 1:  # 두 개 이상의 패킷이 있어야 계산 가능\n",
    "            time_intervals = np.diff(sample)  # 시간 간격 계산\n",
    "            mean_intervals.append(np.mean(time_intervals))  # 평균 계산\n",
    "        else:\n",
    "            mean_intervals.append(0)  # 간격 계산 불가능한 경우 0 추가\n",
    "    return np.array(mean_intervals)\n",
    "\n",
    "def create_features(X1, X2):\n",
    "    X = []\n",
    "    for i in range(len(X1)):\n",
    "        packet_size_direction = sum(X2[i])\n",
    "        cumulative_packet_size = np.sum([abs(c) for c in X2[i]])\n",
    "        burst_lengths = len([c for c in X2[i] if c != 0])\n",
    "\n",
    "        num_incoming_packets = len([c for c in X2[i] if c > 0])\n",
    "        ratio_incoming_packets = num_incoming_packets / len(X2[i]) if len(X2[i]) > 0 else 0\n",
    "        num_outgoing_packets = len([c for c in X2[i] if c < 0])\n",
    "        total_packet_count = len(X2[i])\n",
    "\n",
    "        feature_vector = [\n",
    "            packet_size_direction,\n",
    "            np.mean(X1[i]) if len(X1[i]) > 0 else 0,\n",
    "            cumulative_packet_size,\n",
    "            burst_lengths,\n",
    "            num_incoming_packets,\n",
    "            ratio_incoming_packets,\n",
    "            num_outgoing_packets,\n",
    "            total_packet_count\n",
    "        ]\n",
    "        X.append(feature_vector)\n",
    "    return np.array(X)\n",
    "\n",
    "\n",
    "# 피처 생성\n",
    "X = create_features(X1, X2)\n",
    "\n",
    "# 시간 간격 평균 피처 추가\n",
    "mean_time_intervals = calculate_mean_time_intervals(X1)\n",
    "X = np.hstack((X, mean_time_intervals.reshape(-1, 1)))  # 새로운 피처 결합\n",
    "\n",
    "# 레이블 변환\n",
    "y = np.array(y)\n",
    "\n",
    "# 데이터 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# 데이터 스케일링\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Random Forest 모델 생성 및 학습 (주어진 하이퍼파라미터 사용)\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=30,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    bootstrap=True,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "gb_model = GradientBoostingClassifier(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.001,\n",
    "    max_depth=3,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "svm_model = SVC(\n",
    "    C = 100,\n",
    "    gamma=1,\n",
    "    kernel='rbf',\n",
    "    probability=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# VotingClassifier 정의\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('random_forest', rf_model),\n",
    "        ('gradient_boosting', gb_model),\n",
    "        ('svm', svm_model)\n",
    "    ],\n",
    "    voting='soft',\n",
    "    weights=[2, 1, 2]\n",
    ")\n",
    "\n",
    "# Voting Classifier 학습\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "# 모델 평가\n",
    "y_pred = voting_clf.predict(X_test)\n",
    "\n",
    "# 평가 지표 출력\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unmonitored 데이터 라벨: {np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(7), np.int64(8), np.int64(9), np.int64(10), np.int64(11), np.int64(12), np.int64(13), np.int64(14), np.int64(15), np.int64(16), np.int64(17), np.int64(18), np.int64(19), np.int64(20), np.int64(21), np.int64(22), np.int64(23), np.int64(24), np.int64(25), np.int64(26), np.int64(27), np.int64(28), np.int64(29), np.int64(30), np.int64(31), np.int64(32), np.int64(33), np.int64(34), np.int64(35), np.int64(36), np.int64(37), np.int64(38), np.int64(39), np.int64(40), np.int64(41), np.int64(42), np.int64(43), np.int64(44), np.int64(45), np.int64(46), np.int64(47), np.int64(48), np.int64(49), np.int64(50), np.int64(51), np.int64(52), np.int64(53), np.int64(54), np.int64(55), np.int64(56), np.int64(57), np.int64(58), np.int64(59), np.int64(60), np.int64(61), np.int64(62), np.int64(63), np.int64(64), np.int64(65), np.int64(66), np.int64(67), np.int64(68), np.int64(69), np.int64(70), np.int64(71), np.int64(72), np.int64(73), np.int64(74), np.int64(75), np.int64(76), np.int64(77), np.int64(78), np.int64(79), np.int64(80), np.int64(81), np.int64(82), np.int64(83), np.int64(84), np.int64(85), np.int64(86), np.int64(87), np.int64(88), np.int64(89), np.int64(90), np.int64(91), np.int64(92), np.int64(93), np.int64(94)}\n"
     ]
    }
   ],
   "source": [
    "# Unmonitored 데이터 라벨 확인\n",
    "print(f\"mon 데이터 라벨: {set(y)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 라벨 분포 확인:\n",
      "0     20\n",
      "60    20\n",
      "69    20\n",
      "68    20\n",
      "67    20\n",
      "      ..\n",
      "29    20\n",
      "28    20\n",
      "27    20\n",
      "26    20\n",
      "94    20\n",
      "Name: count, Length: 95, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 결과 평가\n",
    "print(\"데이터 라벨 분포 확인:\")\n",
    "print(pd.Series(y).value_counts())  # 라벨 분포 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 검증 시작 ---\n",
      "Monitored 데이터 라벨링: 정상 (0 ~ 94)\n",
      "--- 검증 완료 ---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Monitored와 Unmonitored 데이터의 라벨 분포 확인\n",
    "def validate_labels(y_mon):\n",
    "    \"\"\"\n",
    "    Monitored와 Unmonitored 데이터의 라벨 분포 검증\n",
    "    :param y_mon: Monitored 데이터 라벨 리스트\n",
    "    :param y_unmon: Unmonitored 데이터 라벨 리스트\n",
    "    :return: 검증 결과 출력\n",
    "    \"\"\"\n",
    "    print(\"\\n--- 검증 시작 ---\")\n",
    "    \n",
    "    # Monitored 라벨 검증\n",
    "    mon_unique_labels = set(y_mon)\n",
    "    if mon_unique_labels == set(range(95)):  # 0 ~ 94\n",
    "        print(\"Monitored 데이터 라벨링: 정상 (0 ~ 94)\")\n",
    "    else:\n",
    "        print(f\"Monitored 데이터 라벨링 오류: {mon_unique_labels}\")\n",
    "    \n",
    "    print(\"--- 검증 완료 ---\\n\")\n",
    "\n",
    "# Monitored와 Unmonitored 데이터 각각 분리\n",
    "monitored_labels = y_mon  # Monitored 라벨 리스트 \n",
    "\n",
    "# 검증 함수 실행\n",
    "validate_labels(monitored_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94]\n"
     ]
    }
   ],
   "source": [
    "print(y_mon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# 라벨 분포 시각화 함수\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot_label_distribution\u001b[39m(y_mon):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 라벨 분포 시각화 함수\n",
    "def plot_label_distribution(y_mon):\n",
    "    \"\"\"\n",
    "    Monitored와 Unmonitored 데이터의 라벨 분포 시각화\n",
    "    :param y_mon: Monitored 데이터 라벨 리스트\n",
    "    :param y_unmon: Unmonitored 데이터 라벨 리스트\n",
    "    \"\"\"\n",
    "    # Monitored 데이터 라벨 분포\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Monitored 라벨 분포 히스토그램\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(y_mon, bins=len(set(y_mon)), color='blue', alpha=0.7, edgecolor='black')\n",
    "    plt.title(\"Monitored Label Distribution\")\n",
    "    plt.xlabel(\"Labels (0 to 94)\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.xticks(range(0, 95, 10))  # x축 간격 설정\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "    \n",
    "    # 그래프 표시\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 시각화 함수 실행\n",
    "plot_label_distribution(monitored_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monitored 데이터 샘플 수: 950\n"
     ]
    }
   ],
   "source": [
    "# Monitored 데이터 샘플 수 확인\n",
    "num_monitored_samples = len(mon_data)\n",
    "print(f\"Monitored 데이터 샘플 수: {num_monitored_samples}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
